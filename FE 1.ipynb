{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f355eee9-445d-49c2-b4c1-ad0085788943",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057b2d4-7a9b-4e66-9cdf-8326b09f7ded",
   "metadata": {},
   "source": [
    "The Filter method in feature selection is a preprocessing step used to select relevant features based on their intrinsic properties without involving any machine learning algorithm. Here's how it works:\n",
    "\n",
    "1)Evaluation Metric:\n",
    "\n",
    "Features are evaluated individually using statistical measures such as correlation, mutual information, chi-square, or ANOVA.\n",
    "\n",
    "2)Scoring:\n",
    "\n",
    "Each feature is assigned a score based on its relevance to the target variable.\n",
    "\n",
    "3)Ranking:\n",
    "\n",
    "Features are ranked according to their scores.\n",
    "\n",
    "4)Selection:\n",
    "\n",
    "A subset of top-ranked features is selected for model training.\n",
    "\n",
    "5)Independence:\n",
    "\n",
    "The Filter method operates independently of any specific model, making it computationally efficient and fast.\n",
    "\n",
    "6)Advantages:\n",
    "\n",
    "Simple to implement and interpret.\n",
    "Reduces overfitting by removing irrelevant features.\n",
    "\n",
    "7)Disadvantages:\n",
    "\n",
    "May overlook feature interactions that are relevant to the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f69f3-5bb6-46f0-a55d-1fb7127a5412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd006007-93fb-414c-a076-e67987efe7bc",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbf905-b863-423a-9552-6461699c5ef2",
   "metadata": {},
   "source": [
    "The Wrapper method in feature selection differs from the Filter method in several key ways:\n",
    "\n",
    "1)Dependency on Model:\n",
    "\n",
    "The Wrapper method evaluates feature subsets by training and testing a specific machine learning model.\n",
    "The Filter method evaluates features independently of any model.\n",
    "\n",
    "2)Evaluation Process:\n",
    "\n",
    "The Wrapper method uses iterative search strategies like forward selection, backward elimination, or recursive feature elimination to find the best feature subset.\n",
    "The Filter method uses statistical measures to score and rank features.\n",
    "\n",
    "3)Performance:\n",
    "\n",
    "The Wrapper method often provides better feature subsets tailored to the specific model, potentially leading to improved performance.\n",
    "The Filter method is generally faster and computationally efficient but might miss feature interactions.\n",
    "\n",
    "4)Computational Cost:\n",
    "\n",
    "The Wrapper method is computationally expensive because it requires repeatedly training the model.\n",
    "The Filter method is less computationally intensive.\n",
    "\n",
    "5)Flexibility:\n",
    "\n",
    "The Wrapper method can capture interactions between features since it evaluates subsets as a whole.\n",
    "The Filter method considers each feature independently, which may miss such interactions.\n",
    "\n",
    "6)Use Cases:\n",
    "\n",
    "The Wrapper method is suitable when computational resources allow for thorough exploration.\n",
    "The Filter method is useful for quick, initial feature selection or when resources are limited.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b180155-3042-4189-872a-ee220ca0b8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9479502c-10b5-4972-a4ed-98ee809cde8d",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bb711-fbbb-4801-8d50-fd75923dfcb7",
   "metadata": {},
   "source": [
    "Embedded feature selection methods integrate the feature selection process into the model training itself. Here are some common techniques used in embedded feature selection:\n",
    "\n",
    "1)LASSO (Least Absolute Shrinkage and Selection Operator):\n",
    "\n",
    "Uses L1 regularization to penalize the absolute size of coefficients, driving some to zero and effectively selecting features.\n",
    "\n",
    "2)Ridge Regression:\n",
    "\n",
    "Uses L2 regularization to penalize the squared size of coefficients, which helps in selecting features by shrinking less important ones but doesn't set them to zero.\n",
    "\n",
    "3)Elastic Net:\n",
    "\n",
    "Combines L1 and L2 regularization to balance the strengths of LASSO and Ridge Regression for feature selection.\n",
    "\n",
    "4)Decision Tree-Based Methods:\n",
    "\n",
    "Decision trees, Random Forests, and Gradient Boosting Trees inherently perform feature selection by splitting nodes based on feature importance.\n",
    "\n",
    "5)Regularized Logistic Regression:\n",
    "\n",
    "Uses L1 or L2 regularization to select features while training a logistic regression model.\n",
    "\n",
    "6)Support Vector Machines (SVM) with L1 Penalty:\n",
    "\n",
    "Employs L1 regularization to select features when training an SVM classifier.\n",
    "\n",
    "7)Recursive Feature Elimination (RFE):\n",
    "\n",
    "Iteratively removes less important features based on model performance, often used with decision trees or linear models.\n",
    "\n",
    "These methods allow for feature selection to be naturally incorporated into the learning process, making them efficient and model-specific.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebc151-f398-40ea-8474-026160234c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3147971-c4a2-4215-9f93-3abf987ebbf4",
   "metadata": {},
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf4f54-4d5d-4d35-95df-fdd21c68a5a7",
   "metadata": {},
   "source": [
    "Here are some drawbacks of using the Filter method for feature selection:\n",
    "\n",
    "1)Lack of Model Interaction:\n",
    "\n",
    "Evaluates features independently of the model, potentially missing interactions between features that are important for the model's performance.\n",
    "\n",
    "2)Ignoring Feature Dependencies:\n",
    "\n",
    "Considers each feature individually, which can lead to suboptimal feature sets if the predictive power comes from combinations of features.\n",
    "\n",
    "3)Fixed Criteria:\n",
    "\n",
    "Relies on statistical measures that may not capture the true relevance of features for the specific problem, leading to the selection of irrelevant features.\n",
    "\n",
    "4)Model Agnosticism:\n",
    "\n",
    "Does not account for how different models might perform with different feature sets, possibly leading to reduced performance for certain algorithms.\n",
    "\n",
    "5)Simplistic Approach:\n",
    "\n",
    "May oversimplify the feature selection process, especially for complex datasets with non-linear relationships.\n",
    "\n",
    "6)Overlooking Non-linear Relationships:\n",
    "\n",
    "May not effectively capture non-linear relationships between features and the target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec6922-7cb1-421f-b11a-e4a904d2462f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a6af1f7-f7e2-4506-9ada-4698621bb78a",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4389d6-1979-464d-b3b2-86339b9bb117",
   "metadata": {},
   "source": [
    "You might prefer using the Filter method over the Wrapper method for feature selection in the following situations:\n",
    "\n",
    "1)Large Datasets:\n",
    "\n",
    "When dealing with large datasets where computational efficiency is crucial, as the Filter method is faster and less resource-intensive.\n",
    "\n",
    "2)Initial Feature Screening:\n",
    "\n",
    "For a quick, preliminary feature selection to reduce the feature space before applying more complex methods.\n",
    "\n",
    "3)High-Dimensional Data:\n",
    "\n",
    "In high-dimensional settings where the number of features is much larger than the number of samples, making Wrapper methods impractical.\n",
    "\n",
    "4)Computational Constraints:\n",
    "\n",
    "When computational resources are limited and model training needs to be efficient.\n",
    "\n",
    "5)Independence from Specific Models:\n",
    "\n",
    "When you want a feature selection process that is independent of any particular machine learning model.\n",
    "\n",
    "6)Simplicity and Interpretability:\n",
    "\n",
    "When you prioritize a simple and interpretable feature selection process over potentially better-performing but more complex alternatives.\n",
    "\n",
    "7)Baseline Feature Selection:\n",
    "\n",
    "When establishing a baseline feature set for comparison with more sophisticated selection methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9c10f-7ad9-428e-a25d-454f74d35333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70791efd-d4fa-400d-8752-80f025e05495",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da51640-12c2-49f1-8c62-73763fcb3aff",
   "metadata": {},
   "source": [
    "When developing a predictive model for customer churn using the Filter Method for feature selection, you can follow these steps to choose the most pertinent attributes:\n",
    "\n",
    "1)Understand the Dataset:\n",
    "\n",
    "Begin by familiarizing yourself with the dataset and the features it contains.\n",
    "Identify the target variable, which in this case is customer churn.\n",
    "\n",
    "2)Data Preprocessing:\n",
    "\n",
    "Handle missing values by imputing or removing them.\n",
    "Normalize or standardize the data if necessary, particularly for features that will be evaluated using distance-based metrics.\n",
    "\n",
    "3)Feature Evaluation:\n",
    "\n",
    "--Choose appropriate statistical measures based on the type of data and the relationship you expect with the target         variable:\n",
    "          a)Correlation Coefficient: For continuous features, calculate the Pearson or Spearman correlation with the                                          target variable.\n",
    "          b)Chi-Square Test: For categorical features, perform a chi-square test to evaluate the independence of each                                  feature with the target variable.\n",
    "          c)Mutual Information: Calculate mutual information for both categorical and continuous features to assess the                                   dependency between features and the target variable.\n",
    "          d)ANOVA F-Test: Use ANOVA for evaluating continuous features against a categorical target.\n",
    "\n",
    "4)Feature Scoring and Ranking:\n",
    "\n",
    "Assign scores to each feature based on the chosen statistical measures.\n",
    "Rank features according to their scores, with higher scores indicating a stronger relationship with the target variable.\n",
    "\n",
    "5)Feature Selection:\n",
    "\n",
    "Set a threshold or choose a fixed number of top-ranked features to select for model training.\n",
    "Consider domain knowledge and business context to adjust the selection of features if necessary.\n",
    "\n",
    "6)Validation and Analysis:\n",
    "\n",
    "Train a baseline model using the selected features to evaluate their effectiveness.\n",
    "Analyze the model's performance on validation data to ensure that selected features contribute positively to predictive accuracy.\n",
    "\n",
    "7)Iterate and Refine:\n",
    "\n",
    "Iterate through the feature selection process, experimenting with different thresholds or statistical measures.\n",
    "Incorporate insights from domain experts to refine feature selection further.\n",
    "\n",
    "By following these steps, you can effectively utilize the Filter Method to select the most relevant features for predicting customer churn, ensuring a balance between model simplicity and predictive power.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a03b7e-5791-4904-b07e-a14d36b4b939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c08339c-2001-4246-adc7-5a9662818625",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50431e7-8021-4a03-b55b-a63e3940049c",
   "metadata": {},
   "source": [
    "Using the Embedded method for feature selection in a soccer match outcome prediction project involves integrating feature selection directly into the model training process. Embedded methods aim to identify the most relevant features while the model is being trained, optimizing both the model's performance and the feature subset simultaneously. Here's how you could use the Embedded method to select the most relevant features for your soccer match outcome prediction model:\n",
    "\n",
    "1)Data Preprocessing: Clean and preprocess the dataset. Handle missing values, encode categorical variables, and ensure that all features are properly formatted.\n",
    "\n",
    "2)Feature Engineering: Based on domain knowledge and insights, create relevant features that could potentially influence the outcome of a soccer match. These could include player statistics, team rankings, historical performance, etc.\n",
    "\n",
    "3)Choose a Model with Built-in Feature Selection: Select a machine learning algorithm that inherently performs feature selection during its training process. Some algorithms, such as Regularized Linear Models (e.g., Lasso or Ridge Regression), Tree-based models (e.g., Random Forest, Gradient Boosting), and some types of Neural Networks, have built-in mechanisms to handle feature selection.\n",
    "\n",
    "4)Train the Model: Split your dataset into training and validation sets. Train the chosen machine learning algorithm on the training set, using all available features. During the training process, the algorithm will automatically assign weights or importance scores to features based on their contribution to the model's performance.\n",
    "\n",
    "5)Evaluate Feature Importance: After training the model, you can evaluate the importance of features using the weights or importance scores assigned by the algorithm. Different algorithms provide different ways to access feature importance, such as coefficients for linear models or feature importances for tree-based models.\n",
    "\n",
    "6)Select Relevant Features: Based on the calculated feature importance scores, select the most relevant features. You can set a threshold to determine which features to keep. Alternatively, you can select the top N features with the highest importance scores.\n",
    "\n",
    "7)Re-train the Model: Retrain the model using only the selected relevant features. This streamlined feature subset will likely improve model performance and reduce overfitting since the model focuses on the most influential attributes.\n",
    "\n",
    "8)Validate and Tune: Validate the model's performance on a separate validation set. Fine-tune hyperparameters if necessary. The embedded feature selection method helps ensure that the model is trained on the most informative features, leading to better generalization.\n",
    "\n",
    "9)Interpret Results: Interpret the model's results in terms of the selected features. This analysis can provide insights into which player statistics or team rankings have the most impact on predicting soccer match outcomes.\n",
    "\n",
    "10)Iterative Refinement: Depending on the model's performance and insights gained, you can iteratively refine the feature selection process. Experiment with different algorithms, hyperparameters, and subsets of features to find the optimal combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39ac5-70f5-4548-9b09-e3844c18c10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a8c4f4d-3782-4d42-af4a-16b236ded86e",
   "metadata": {},
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36fae8-c71e-42b9-af98-5476d07f50ae",
   "metadata": {},
   "source": [
    "Using the Wrapper method for feature selection in your house price prediction project involves training and evaluating your predictive model iteratively with different subsets of features. The goal is to identify the best set of features that result in the optimal performance of the model. Here's how you could use the Wrapper method to select the best set of features for your predictor:\n",
    "\n",
    "1)Data Preprocessing: Clean and preprocess the dataset. Handle missing values, encode categorical variables, and ensure that the data is ready for training.\n",
    "\n",
    "2)Initial Feature Subset: Start with an initial feature subset. This could be all available features or a subset that you believe are most relevant based on domain knowledge.\n",
    "\n",
    "3)Model Selection: Choose a machine learning algorithm to use for your house price prediction. This could be a regression algorithm such as Linear Regression, Decision Tree Regression, or even a more complex model like Random Forest or Gradient Boosting.\n",
    "\n",
    "4)Train and Evaluate Initial Model: Train the selected model using the initial feature subset and evaluate its performance using a suitable metric such as Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). This initial performance serves as a baseline.\n",
    "\n",
    "5)Wrapper Iteration: Perform the following iterative process to select the best set of features:\n",
    "\n",
    "a. Feature Subset Generation: Start with the initial feature subset.\n",
    "\n",
    "b. Model Training and Evaluation: Train the model using the current feature subset and evaluate its performance.\n",
    "\n",
    "c. Feature Selection/Removal: Depending on the wrapper method strategy (forward selection, backward elimination, recursive feature elimination), add or remove features from the current subset.\n",
    "\n",
    "d. Iteration: Repeat steps b and c for different subsets of features, evaluating the model's performance each time.\n",
    "\n",
    "6)Performance Comparison: For each iteration, compare the model's performance on the validation or cross-validation set. You can use metrics like MSE or RMSE to quantify the prediction error.\n",
    "\n",
    "7)Select Best Feature Subset: Identify the feature subset that led to the best model performance based on the chosen metric. This subset of features will be the final set you use for building your predictor.\n",
    "\n",
    "8)Model Fine-Tuning: After selecting the best feature subset, you can fine-tune hyperparameters of your chosen model for optimal performance. This step is important to ensure the best predictive accuracy.\n",
    "\n",
    "9)Final Model Evaluation: Evaluate the final model using a separate test set that the model has never seen before. This step gives you a realistic estimate of how well your model performs on unseen data.\n",
    "\n",
    "10)Interpretation: Interpret the results by analyzing the selected feature subset. This analysis can provide insights into which features have the most impact on predicting house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcbd6c-59d7-4d02-b5a2-366cc1f2a98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e6df0-dc21-46d6-8124-ebabfc8f427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfe42c-591e-4db4-96a0-6a7c995dcba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276db6c5-6089-451b-b38a-f283a84c17fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16588c9d-813e-4897-88b0-ea0fa5c07d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c5461-2d9f-42fb-8857-e40eadbdb33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8b9a3-d0c4-4d95-9910-572f468888ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a368bc-b60b-40da-9385-abc9678a6aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0ea99-38ae-45ba-ba8d-6aa74430c9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd2b39-1a66-42c5-b9ad-73ada0c9ddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404bbcb6-1d57-474d-9745-cb44f1f56f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7680a4-e3eb-49fa-8b7f-e63d5bd93a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198ed09-388d-4de1-9703-88656817ab99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3540d9-d5ab-432e-b113-2f9534e0d2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637dfd4d-e51b-4ec2-9c15-237d85bdaecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8bba9-d2be-45ca-a14a-e2965052412e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ae376-7c2b-428b-a817-388fafe3b36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224f89b-7a02-40f2-a160-1be3ff5960da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6e4ee-75fa-47c7-b22a-1a15d48c94fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b0df0-9811-44ce-ba61-43f3d46f5edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3bf30-3c8d-4f95-94ef-521e04f20b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68753855-372a-4768-a284-84baf78622fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829e9fc-e44f-4e04-a435-4156bc6a8525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833fdc5-4a47-4b93-9649-e34d3c658fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21af32-630c-454f-baac-c90ea0afdc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b6e91-a317-4d1f-8fbc-83207dd50ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac565a-21a9-4054-9297-9ad2ace5cbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808fedc-b333-408f-8dd5-d397872fe5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc05059-c3ca-4bea-94b2-60d36b99887e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536a05c-db18-4615-98f9-7cdb6d832043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4825b62-3313-461d-8134-293c4b687a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988518d-cec3-4505-9678-67c4a3f2e553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e2d2b-4206-46a3-83ae-a4e05e3f3b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867644c5-edd8-4724-bfdb-27805eaca320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01995452-d9e8-4ad7-bb19-a424ac2197c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b88653-fd85-4ae5-a611-228768e9c407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec00ab1-b43f-4afb-a519-e2babdef076c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc536bf6-347d-4267-b00c-f52c72554750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3909b4-9e83-4f69-b2cc-3583b4dfabae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
